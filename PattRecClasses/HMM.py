import numpy as np
from .DiscreteD import DiscreteD
from .GaussD import GaussD
from .MarkovChain import MarkovChain

class HMM:
    """
    HMM - class for Hidden Markov Models, representing
    statistical properties of random sequences.
    Each sample in the sequence is a scalar or vector, with fixed DataSize.
    
    Several HMM objects may be collected in a single multidimensional array.
    
    A HMM represents a random sequence(X1,X2,....Xt,...),
    where each element Xt can be a scalar or column vector.
    The statistical dependence along the (time) sequence is described
    entirely by a discrete Markov chain.
    
    A HMM consists of two sub-objects:
    1: a State Sequence Generator of type MarkovChain
    2: an array of output probability distributions, one for each state
    
    All states must have the same class of output distribution,
    such as GaussD, GaussMixD, or DiscreteD, etc.,
    and the set of distributions is represented by an object array of that class,
    although this is NOT required by general HMM theory.
    
    All output distributions must have identical DataSize property values.
    
    Any HMM output sequence X(t) is determined by a hidden state sequence S(t)
    generated by an internal Markov chain.
    
    The array of output probability distributions, with one element for each state,
    determines the conditional probability (density) P[X(t) | S(t)].
    Given S(t), each X(t) is independent of all other X(:).
    
    
    References:
    Leijon, A. (20xx) Pattern Recognition. KTH, Stockholm.
    Rabiner, L. R. (1989) A tutorial on hidden Markov models
    	and selected applications in speech recognition.
    	Proc IEEE 77, 257-286.
    
    """
    def __init__(self, mc, distributions):
        self.stateGen = mc
        self.outputDistr = distributions
        self.nStates = mc.nStates
        self.dataSize = distributions[0].dataSize

    @staticmethod
    def log_sum_exp(x, axis=None):
        """Numerically stable log-sum-exp with dimension control"""
        xmax = np.max(x, axis=axis, keepdims=True)
        x = x - xmax
        result = np.log(np.sum(np.exp(x), axis=axis, keepdims=True)) + xmax
        if axis is not None:
            result = np.squeeze(result, axis=axis)
        return result

    def rand(self, nSamples):
        """
        [X,S]=rand(self,nSamples); generates a random sequence of data
        from a given Hidden Markov Model.
        
        Input:
        nSamples=  maximum no of output samples (scalars or column vectors)
        
        Result:
        X= matrix or row vector with output data samples
        S= row vector with corresponding integer state values
          obtained from the self.StateGen component.
          nS= length(S) == size(X,2)= number of output samples.
          If the StateGen can generate infinite-duration sequences,
              nS == nSamples
          If the StateGen is a finite-duration MarkovChain,
              nS <= nSamples
        """
        print(f"[HMM] Generating random sequence of length {nSamples}")
        S = self.stateGen.rand(nSamples)
        nS = len(S)
        
        if self.dataSize == 1:
            X = np.zeros(nS)
        else:
            X = np.zeros((self.dataSize, nS))
        
        for t, state in enumerate(S):
            distr = self.outputDistr[state-1]
            if isinstance(distr, GaussD):
                x = distr.rand(1)
            elif isinstance(distr, DiscreteD):
                x = distr.rand(1)
            else:
                raise ValueError("Unsupported distribution type")
            
            if self.dataSize == 1:
                X[t] = x
            else:
                X[:, t] = x.reshape(-1)
        print("[HMM] Random sequence generation done")
        return X, S

    def log_forward(self, observations):
        """Forward algorithm in log domain for numerical stability"""
        T = len(observations)
        N = self.nStates
        log_alpha = np.zeros((N, T))
        
        # Initialization
        for i in range(N):
            log_alpha[i, 0] = np.log(self.stateGen.q[i] + 1e-20) + \
                              np.log(self.outputDistr[i].prob(observations[0]) + 1e-20)
        
        # Recursion
        for t in range(1, T):
            for j in range(N):
                # Log-sum-exp for numerical stability
                log_trans = np.log(self.stateGen.A[:, j] + 1e-20) + log_alpha[:, t-1]
                log_sum = self.log_sum_exp(log_trans)
                log_alpha[j, t] = log_sum + np.log(self.outputDistr[j].prob(observations[t]) + 1e-20)
        
        return log_alpha

    def log_backward(self, observations):
        """Backward algorithm in log domain for numerical stability"""
        T = len(observations)
        N = self.nStates
        log_beta = np.zeros((N, T))
        
        # Initialize: beta at time T-1 is 1 -> log(1)=0
        log_beta[:, T-1] = 0.0
        
        # Recursion
        for t in range(T-2, -1, -1):
            for i in range(N):
                # Compute for each next state j
                log_terms = np.zeros(N)
                for j in range(N):
                    log_terms[j] = np.log(self.stateGen.A[i, j] + 1e-20) + \
                                  np.log(self.outputDistr[j].prob(observations[t+1]) + 1e-20) + \
                                  log_beta[j, t+1]
                
                # Log-sum-exp for numerical stability
                log_beta[i, t] = self.log_sum_exp(log_terms)
        
        return log_beta

    def train(self, observations_list, n_iter=10):
        for iteration in range(n_iter):
            print(f"[HMM] Training iteration {iteration+1}/{n_iter}")
            
            # Initialize accumulators
            initial_acc = np.zeros(self.nStates)
            trans_acc = np.zeros((self.nStates, self.nStates))
            gamma_acc = [np.zeros(self.dataSize) for _ in range(self.nStates)]
            gamma_count = np.zeros(self.nStates)
            gamma_xxT_acc = [np.zeros((self.dataSize, self.dataSize)) for _ in range(self.nStates)]

            # Process each sequence to accumulate statistics
            for observations in observations_list:
                T = len(observations)
                if T < 2:
                    continue

                log_alpha = self.log_forward(observations)
                log_beta = self.log_backward(observations)
                log_likelihood = self.log_sum_exp(log_alpha[:, T-1])
                
                # Gamma: P(state=i at time t | observations)
                log_gamma = log_alpha + log_beta - log_likelihood
                gamma = np.exp(log_gamma)
                
                # Xi: P(state=i at t, state=j at t+1 | observations)
                log_xi = np.zeros((self.nStates, self.nStates, T-1))
                for t in range(T-1):
                    for i in range(self.nStates):
                        for j in range(self.nStates):
                            log_xi[i, j, t] = (
                                log_alpha[i, t] +
                                np.log(self.stateGen.A[i, j] + 1e-20) +
                                np.log(self.outputDistr[j].prob(observations[t+1]) + 1e-20) +
                                log_beta[j, t+1] -
                                log_likelihood
                            )
                xi = np.exp(log_xi)
                
                # Accumulate statistics
                initial_acc += gamma[:, 0]
                trans_acc += np.sum(xi, axis=2)
                
                for j in range(self.nStates):
                    gamma_count[j] += np.sum(gamma[j, :])
                    gamma_acc[j] += np.sum(observations * gamma[j, :, None], axis=0)
                    diff = observations - self.outputDistr[j].means
                    gamma_xxT_acc[j] += np.dot((diff * gamma[j, :, None]).T, diff)

            # Update initial probabilities
            self.stateGen.q = initial_acc / np.sum(initial_acc)
            
            # Update transition matrix
            row_sums = np.sum(trans_acc, axis=1, keepdims=True)
            self.stateGen.A = trans_acc / np.where(row_sums == 0, 1e-10, row_sums)
            
            # Update emission distributions
            for j in range(self.nStates):
                if gamma_count[j] > 0:
                    # Update means
                    self.outputDistr[j].means = gamma_acc[j] / gamma_count[j]
                    
                    # Update covariance (ensure diagonal)
                    cov = gamma_xxT_acc[j] / gamma_count[j]
                    if not np.allclose(cov, np.diag(np.diag(cov))):
                        cov = np.diag(np.diag(cov))
                    self.outputDistr[j].cov = cov
                    self.outputDistr[j].stdevs = np.sqrt(np.diag(cov))
                    self.outputDistr[j].variance = np.diag(cov)
            print(f"[HMM] Updated transition matrix: \n{self.stateGen.q} {self.stateGen.A}")

    def viterbi(self, observations):
        print("[HMM] Starting Viterbi decoding")
        T = len(observations)
        N = self.nStates
        delta = np.zeros((N, T))
        psi = np.zeros((N, T), dtype=int)

        delta[:, 0] = np.log(self.stateGen.q + 1e-10) + \
                     np.array([d.prob(observations[0]) for d in self.outputDistr])
        delta[:, 0] = np.where(delta[:, 0] < -1e20, -1e20, delta[:, 0])

        for t in range(1, T):
            for j in range(N):
                trans_prob = np.log(self.stateGen.A[:, j] + 1e-10)
                prev = delta[:, t-1] + trans_prob
                psi[j, t] = np.argmax(prev)
                delta[j, t] = prev[psi[j, t]] + np.log(self.outputDistr[j].prob(observations[t]) + 1e-10)

        states = np.zeros(T, dtype=int)
        states[-1] = np.argmax(delta[:, -1])
        for t in range(T-2, -1, -1):
            states[t] = psi[states[t+1], t+1]

        print("[HMM] Viterbi decoding finished")
        return states + 1  # 1-based states

    def logprob(self, observations):
        _, c = self.forward(observations)
        lp = -np.sum(np.log(c))
        return lp
